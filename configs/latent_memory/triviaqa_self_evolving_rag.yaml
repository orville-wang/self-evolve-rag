model:
  # base llm
  model_name: Qwen/Qwen2.5-1.5B-Instruct
  load_model_path: null

  # max prompt/inference augmentation num
  max_prompt_aug_num: 8  # single turn
  max_inference_aug_num: 0

  # weaver configs
  weaver:
    model_name: Qwen/Qwen2.5-1.5B-Instruct
    prompt_latents_len: 8
    inference_latents_len: 8

    lora_config:
      r: 16
      lora_alpha: 32
      target_modules: ["q_proj", "v_proj"]
      lora_dropout: 0.1
      bias: "none"
      task_type: "CAUSAL_LM"

  # trigger configs
  trigger:
    model_name: Qwen/Qwen2.5-1.5B-Instruct
    active: False

    lora_config:
      r: 16
      lora_alpha: 32
      target_modules: ["q_proj", "v_proj"]
      lora_dropout: 0.1
      bias: "none"
      task_type: "CAUSAL_LM"

dataset:
    name: triviaqa
    mode: grpo  # Use GRPO for self-evolving training
    sft:
      valid_ratio: 0.1
    grpo:
      valid_ratio: 0.1


# training/evaluation configs
run:

  seed: 42

  # route
  mode: train
  train_weaver: True
  train_weaver_method: grpo    # Use GRPO for reward-based training
  train_trigger: False
  train_trigger_method: grpo

  # processor training configs
  weaver:

    # grpo configs for self-evolving RAG
    grpo:
      num_train_epochs: 1  # Reduce to 1 epoch for testing
      per_device_train_batch_size: 2  # Reduce batch size
      per_device_eval_batch_size: 2
      num_generations: 4  # Reduce generations
      num_iterations: 1
      gradient_accumulation_steps: 2  # Effective batch size = 4
      beta: 0.01  # Small KL penalty
      loss_type: grpo

      max_prompt_length: 256  # Match with max_start_length
      max_completion_length: 128  # Reduce completion length
      temperature: 0.7  # Slightly lower for more focused generation

      # optimizer configs
      optim: adamw_torch
      lr_scheduler_type: cosine
      warmup_ratio: 0.1
      learning_rate: 5e-6  # Lower LR for stable training

      # duration
      logging_strategy: steps
      logging_steps: 10
      eval_strategy: epoch
      eval_steps: 100
      save_strategy: epoch
      save_steps: 100

      remove_unused_columns: False
      load_best_model_at_end: True
      bf16: True
      report_to:
        - tensorboard

  # interaction config for GRPO training and evaluation
  interaction:
    max_turns: 5
    max_start_length: 256
    max_prompt_length: 4096
    max_response_length: 128  # Match with max_completion_length
    max_obs_length: 512
    temperature: 0.7
    batch_size: 4

# Self-evolving RAG memory configuration
memory:
  enable: true
  store_path: /root/autodl-tmp/phase0_experience.jsonl
  index_type: simple
  topk: 1  # Retrieve top-1 most similar experience
  min_score: 0.3  # Minimum similarity threshold
  writeback:
    enable: true
    min_reward: 0.01  # Lower threshold to collect more experiences
    require_grounding: false  # Don't require grounding for TriviaQA
